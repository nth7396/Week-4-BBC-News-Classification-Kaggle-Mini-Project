{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Link: https://colab.research.google.com/drive/1gRwlTisVyPubmpGJwRewtciSTIx7QQZb?usp=sharing"
      ],
      "metadata": {
        "id": "CPNKpgQgrkb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1."
      ],
      "metadata": {
        "id": "FKKtTqKWJuZh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjsRC1zlFBum",
        "outputId": "da98c45f-4501-4414-ffec-da3922f96019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "uBODqKzjF1b8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "MV_users = pd.read_csv('/content/gdrive/MyDrive/users.csv')\n",
        "MV_movies = pd.read_csv('/content/gdrive/MyDrive/movies.csv')\n",
        "train = pd.read_csv('/content/gdrive/MyDrive/train.csv')\n",
        "test = pd.read_csv('/content/gdrive/MyDrive/test.csv')"
      ],
      "metadata": {
        "id": "AfMzJu1KIda_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pivot train and test DataFrames to matrices\n",
        "train_matrix = train.pivot(index='uID', columns='mID', values='rating')\n",
        "test_matrix = test.pivot(index='uID', columns='mID', values='rating')"
      ],
      "metadata": {
        "id": "0r7umXY-IlqQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values - here we fill NaNs with zeros\n",
        "# Adjust this as needed\n",
        "train_matrix_filled = train_matrix.fillna(0)"
      ],
      "metadata": {
        "id": "MGFxO-ssIm7F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "model = NMF(n_components=20, init='random', random_state=0)"
      ],
      "metadata": {
        "id": "E7HztTOgIosg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the train data and obtain W and H matrices\n",
        "W = model.fit_transform(train_matrix_filled)\n",
        "H = model.components_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcTrSrytIqSr",
        "outputId": "ae9c9a7b-b1f7-4cfb-c98d-168d74f0fbc8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the predicted ratings\n",
        "predicted_ratings = np.dot(W, H)\n",
        "\n",
        "# Convert the predicted ratings to a DataFrame with matching index and columns\n",
        "predicted_ratings_df = pd.DataFrame(predicted_ratings, index=train_matrix.index, columns=train_matrix.columns)"
      ],
      "metadata": {
        "id": "ZZG4lsOEIsvd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, you can use `predicted_ratings_df` to find the predicted rating for any user-item pair,\n",
        "# including those that are missing in the original data.\n",
        "\n",
        "# For evaluation, let's calculate RMSE for the known ratings in the test set.\n",
        "\n",
        "# Align the shape of the predicted_ratings_df with test_matrix\n",
        "predicted_ratings_aligned = predicted_ratings_df.reindex_like(test_matrix)"
      ],
      "metadata": {
        "id": "MDYhLmLvI1Ut"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify indices where test matrix has actual ratings\n",
        "indices = np.where(~np.isnan(test_matrix))\n",
        "\n",
        "# Extract the corresponding predictions and actual values\n",
        "predicted_ratings_masked = predicted_ratings_aligned.values[indices]\n",
        "actual_ratings = test_matrix.values[indices]\n",
        "\n",
        "# Drop any pairs where either the prediction or the actual value is NaN\n",
        "non_nan_indices = np.where(~(np.isnan(predicted_ratings_masked) | np.isnan(actual_ratings)))\n",
        "\n",
        "predicted_ratings_masked = predicted_ratings_masked[non_nan_indices]\n",
        "actual_ratings = actual_ratings[non_nan_indices]\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(actual_ratings, predicted_ratings_masked))\n",
        "\n",
        "print('RMSE: ', rmse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7lySGdtJEp-",
        "outputId": "35e38502-2126-4792-b676-f3e45f30f613"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE:  2.865942566224596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " The RMSE above indicates, on average, how far off the model's predicted ratings are from the actual ratings. An RMSE of 2.86 implies that, on average, the predicted ratings by your model are about 2.86 points off from the actual ratings. Given that the ratings range from 1 to 5, this is a relatively high error."
      ],
      "metadata": {
        "id": "EvtsxmpLhshH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2."
      ],
      "metadata": {
        "id": "6OrCIs-qJrYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|Method|RMSE|\n",
        "|:----|:--------:|\n",
        "|Baseline, $Y_p$=3|1.35 |\n",
        "|Baseline, $Y_p=\\mu_u$|1.20 |\n",
        "|Content based, item-item|0.98 |\n",
        "|Collaborative, cosine|0.90 |\n",
        "|Collaborative, jaccard, $M_r\\geq 3$|0.95  |\n",
        "|Collaborative, jaccard, $M_r\\geq 1$|0.92  |\n",
        "|Collaborative, jaccard, $M_r$|1.05  |\n",
        "|Sklearn NMF |2.87  |"
      ],
      "metadata": {
        "id": "lBxm9S8iJtfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that the NMF performs the worst compared to other methods. One of the reasons for this can be because of the missing data. The ratings matrix in a typical movie ratings dataset is sparse, with a lot of missing entries because not all users have rated all movies. Methods like NMF might struggle with such sparse data, while similarity-based methods and collaborative filtering can handle this sparsity more effectively.\n",
        "\n",
        "One possible solution is to introduce Regularization. Regularization can help mitigate overfitting, which is a common problem when dealing with sparse data. Adding L1 or L2 regularization to the cost function of NMF can help prevent overfitting. Or another option is to research for specialized libraries for building recommendation systems such as Surprise (https://surpriselib.com)."
      ],
      "metadata": {
        "id": "7lraOiZYMzds"
      }
    }
  ]
}